# LLM API配置
llm:
  api_url: "http://localhost:11434/v1/chat/completions"  # API地址
  model_name: "deepseek-r1:latest"  # 模型名称
  api_key: ""  # API密钥（如果需要）
  timeout: 3000  # 请求超时时间（秒）增加到3000秒
  max_retries: 5  # 最大重试次数增加到5次
  temperature: 0.7  # 温度参数

# 文件路径配置
paths:
  input_dir: "file"  # 输入文档目录
  output_dir: "output"     # 输出目录

# 文件处理配置
processing:
  supported_extensions:    # 支持的文件类型
    - .md
    - .docx
    - .pdf
    - .xlsx
    - .xls
  questions_per_file: 10   # 每个文件生成的问题总数
  text_chunking:          # 文本分块配置
    max_tokens: 2000      # 每个文本块的最大token数
    overlap_tokens: 200   # 文本块之间的重叠token数
    # 配置说明：
    # 1. max_tokens建议值计算：
    #    - 对于GPT-3.5/4模型：建议设置为模型最大上下文长度的1/4到1/3
    #    - 例如：GPT-3.5的上下文长度为4096，则max_tokens建议设置为1000-1500
    #    - 当前设置为2000，适用于上下文长度为8k的模型
    # 2. overlap_tokens建议值计算：
    #    - 建议设置为max_tokens的5%-15%
    #    - 当前设置为200，约为max_tokens的10%
    # 3. 注意事项：
    #    - 较大的max_tokens可能导致生成的问题不够具体
    #    - 较大的overlap_tokens可以保持上下文连贯性，但会增加处理时间
    #    - 建议根据实际文档内容和模型能力调整这些参数

# 输出配置
output:
  csv_filename_template: "qa_pairs_{timestamp}.csv"  # CSV文件名模板
  excel_filename_template: "qa_pairs_{timestamp}.xlsx"  # Excel文件名模板

# 提示词配置
prompts:
  system_prompt_template: |
    你是一个专业的文档分析助手。你的任务是：
    1. 仔细阅读提供的文档内容
    2. 提出{questions_count}个与文档内容相关的重要问题
    3. 对每个问题，从文档中提取相关信息作为答案
    4. 确保问题和答案都是清晰、准确且相关的
    5. 问题要求：
       - 避免使用"本文件"、"本办法"、"本条例"等模糊指代
       - 使用文档标题或具体名称进行指代
       - 问题要具体、明确，便于读者理解
       - 答案要完整、准确，直接引用原文内容
    6. 示例：
       不好的问题：
       - "本办法规定了哪些内容？"
       - "本条例的适用范围是什么？"
       好的问题：
       - "《XX管理办法》规定了哪些主要内容？"
       - "《XX条例》适用于哪些对象和场景？"
    7. 以JSON格式返回结果，格式为：[{{"question": "问题1", "answer": "答案1"}}, ...]
    8. 只返回JSON格式的数据，不要包含任何其他内容
  user_prompt_template: "请分析以下文档内容并生成问答对。注意：请使用文档标题或具体名称进行指代，避免使用'本文件'、'本办法'、'本条例'等模糊指代。\n\n{text}" 